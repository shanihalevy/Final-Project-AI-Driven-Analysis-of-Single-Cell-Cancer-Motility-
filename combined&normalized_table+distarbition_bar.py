# -*- coding: utf-8 -*-
"""combined&normalized_table+Distarbition_Bar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dTFSQrgb4owDdbDdxP1WXEELnl_iKfk5

# **Data Visualization**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the CSV file into a pandas DataFrame
# Correcting the file path based on the uploaded file name
file_path = "/content/Combined_SummaryTable.csv"
df = pd.read_csv(file_path)

# Inspect the first few rows of the DataFrame
print("First 5 rows of the DataFrame:")
print(df.head())

# Get information about the DataFrame, including column data types and non-null counts
print("\nDataFrame Info:")
df.info()

"""## **Combined & Outliners & Normalized table ****"""

import pandas as pd
# from scipy.stats import zscore # This line is commented out as it's not directly used for manual Z-score calc.

# --- 1. Read the two tables directly from Colab's temporary storage ---
# Assuming your files 'SummeryTableAfterNaNHandling_B3.csv' and
# 'SummeryTableAfterNaNHandling_B6.csv' have already been uploaded
# manually to the Colab runtime environment via the Files tab.
try:
    df1 = pd.read_csv('SummeryTableAfterNaNHandling_B3.csv')
    df2 = pd.read_csv('SummeryTableAfterNaNHandling_B6.csv') # Corrected: removed base_path if reading locally
    print("CSV files loaded successfully from Colab runtime.")
except FileNotFoundError:
    print("Error: Files not found. Please ensure they are uploaded to Colab's runtime and names match exactly.")
    print("Expected files: 'SummeryTableAfterNaNHandling_B3.csv' and 'SummeryTableAfterNaNHandling_B6.csv'.")
    exit() # Exit if files are not found

# Remove the second header row if it was mistakenly saved within the file as a regular row,
# or simply skip the first content row.
df2_no_header = df2.iloc[1:].reset_index(drop=True)

# Concatenate the two tables
combined_df = pd.concat([df1, df2_no_header], ignore_index=True)

# Define columns that should NEVER be normalized or have outliers treated, as they are identifiers or non-features.
# Added 'image_number' to this list.
IDENTIFIER_COLUMNS = ['filename', 'IMAENUMBER', 'ObjectNumber', 'image_number', 'time_point']

# --- Outlier Handling (IQR method) before normalization ---
# Identify numerical columns for outlier treatment.
# Exclude identifier columns from this process.
outlier_cols = combined_df.select_dtypes(include=['number']).columns.tolist()
for id_col in IDENTIFIER_COLUMNS:
    if id_col in outlier_cols:
        outlier_cols.remove(id_col)

for col in outlier_cols:
    Q1 = combined_df[col].quantile(0.25)
    Q3 = combined_df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Replace outliers with boundary values
    combined_df[col] = combined_df[col].apply(lambda x: lower_bound if x < lower_bound else (upper_bound if x > upper_bound else x))

# --- Z-score Normalization per Barit type and per feature (manual calculation) ---
# Create a new column to identify the Barit type
combined_df['Barit_Type'] = combined_df['filename'].apply(lambda x: 'B03' if 'B03' in x else ('B06' if 'B06' in x else 'Unknown'))

# List of features to normalize.
# Explicitly exclude all identifier columns.
features_to_normalize = combined_df.select_dtypes(include=['number']).columns.tolist()
for id_col in IDENTIFIER_COLUMNS:
    if id_col in features_to_normalize:
        features_to_normalize.remove(id_col)

for barit_type in ['B03', 'B06']:
    barit_df = combined_df[combined_df['Barit_Type'] == barit_type]
    for feature in features_to_normalize:
        # Calculate mean and standard deviation for each feature separately for each Barit type
        mean_val = barit_df[feature].mean()
        std_val = barit_df[feature].std()

        # Normalize only if the standard deviation is not zero to prevent division by zero
        if std_val != 0:
            combined_df.loc[combined_df['Barit_Type'] == barit_type, feature] = (barit_df[feature] - mean_val) / std_val
        else:
            # If standard deviation is zero, all values are the same; set to 0
            combined_df.loc[combined_df['Barit_Type'] == barit_type, feature] = 0

# --- Time Synchronization Validation ---
# This step assumes a defined time column (e.g., 'time_point').
# The assumption here is that a column named 'time_point' contains the time points.
time_points_column = 'time_point' # Change this to the correct column name if it's different

if time_points_column in combined_df.columns:
    # Count the number of unique time points for each source file (or other measurement unit)
    time_point_counts = combined_df.groupby('filename')[time_points_column].nunique()

    # Check if each file contains exactly 30 time points
    if all(time_point_counts == 30):
        print("âœ” Time Synchronization Validation: Each file has exactly 30 time points.")
    else:
        print("âš  Warning: Files found without exactly 30 time points. Details:")
        print(time_point_counts[time_point_counts != 30])
else:
    print(f"âš  Warning: Time column '{time_points_column}' not found. Cannot perform time synchronization validation.")

# Remove the auxiliary 'Barit_Type' column if not needed later
combined_df = combined_df.drop(columns=['Barit_Type'])

# --- 3. Save the combined and processed file to Colab's temporary storage ---
# The output file will be saved directly in the Colab runtime environment.
# You will need to download it manually if you want to keep it.
output_file_name = 'Combined_SummaryTable_Processed.csv'
combined_df.to_csv(output_file_name, index=False)

print(f"\nTable successfully processed and saved as '{output_file_name}' in Colab runtime.")
print(f"To download the file, click the folder icon on the left sidebar, locate '{output_file_name}', and click the three dots to download.")

"""# **Distribution - Feature_Frequancy**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# Load the CSV file
#file_path = "/content/Combined_SummaryTable.csv"
#file_path = "/content/Combined_SummaryTable_Processed.csv"
file_path = '/content/Combined_SummaryTable_NoNormalization.csv'
df = pd.read_csv(file_path)

# Drop the 'Unnamed: 0' column as it's an index
if 'Unnamed: 0' in df.columns:
    df = df.drop(columns=['Unnamed: 0'])

# Identify numerical columns for descriptive statistics
# Exclude columns that are likely identifiers, coordinates, or categorical
exclude_cols = ['TimeIndex', 'x_Pos', 'y_Pos', 'Parent', 'dt', 'ID', 'Experiment']
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
parameter_cols = [col for col in numerical_cols if col not in exclude_cols]

# --- Bar Graph of Parameter Means ---
# Calculate the mean of each parameter
parameter_means = df[parameter_cols].mean().sort_values()

plt.figure(figsize=(15, 10)) # Increased figure size for better readability
parameter_means.plot(kind='bar')
plt.title('Mean Values of Parameters')
plt.ylabel('Mean Value')
plt.xlabel('Parameter')
plt.xticks(rotation=90, ha='right') # Rotate labels and align to right
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.savefig("parameter_means_bar_chart.png")
plt.close()

print("Bar chart of parameter means saved as parameter_means_bar_chart.png")

# --- Distribution Plots (Histograms with KDE) for specific parameters ---
# Define the specific parameters to visualize
selected_params_for_dist = [
    'Area Shape',
    'TrackObjects_Displacement_50',
    'Acceleration',
    'TrackObjects_DistanceTraveled_50',
    'TrackObjects_Label_50',
    'TrackObjects_TrajectoryX_50',
    'speed_per_hour',
    'Distance'
]

# Filter to include only the selected parameters that exist in the DataFrame
selected_params_for_dist = [param for param in selected_params_for_dist if param in df.columns]

if selected_params_for_dist:
    num_plots = len(selected_params_for_dist)
    # Adjust subplot layout based on the number of plots
    if num_plots <= 3:
        n_cols_subplot = num_plots
        n_rows_subplot = 1
    elif num_plots <= 6:
        n_cols_subplot = 3
        n_rows_subplot = 2
    else: # For more plots (up to 8 in this case), adjust as needed
        n_cols_subplot = 4
        n_rows_subplot = 2

    plt.figure(figsize=(5 * n_cols_subplot, 4 * n_rows_subplot))
    for i, col in enumerate(selected_params_for_dist):
        plt.subplot(n_rows_subplot, n_cols_subplot, i + 1)
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')
    plt.tight_layout()
    plt.savefig("selected_parameter_distributions.png")
    plt.close()
    print(f"Distribution plots for selected parameters saved as selected_parameter_distributions.png. Selected parameters: {', '.join(selected_params_for_dist)}")

    # Create individual distribution plots for the selected parameters
    if not os.path.exists("individual_distribution_plots"):
        os.makedirs("individual_distribution_plots")

    for col in selected_params_for_dist:
        plt.figure(figsize=(8, 6))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')
        plt.tight_layout()
        plt.savefig(f"individual_distribution_plots/distribution_{col.replace('/', '_')}.png") # Replace slashes for valid filenames
        plt.close()
    print(f"Individual distribution plots for {len(selected_params_for_dist)} selected parameters saved in the 'individual_distribution_plots' directory.")

else:
    print("No selected parameters found to plot distributions.")

# --- Descriptive Statistics Table ---
# Compute descriptive statistics and save them to a CSV file
descriptive_stats_table = df[parameter_cols].describe()
descriptive_stats_table.to_csv("descriptive_statistics_summary.csv")
print("Descriptive statistics summary table saved as descriptive_statistics_summary.csv")

"""×œ×¤×™ ×˜×™××•×œ×™×"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# Load the CSV file
# ×˜×¢×Ÿ ××ª ×§×•×‘×¥ ×”-CSV
file_path = '/content/Combined_SummaryTable_NoNormalization.csv'
df = pd.read_csv(file_path)

# Drop the 'Unnamed: 0' column as it's an index
# ×”×¡×¨ ××ª ×”×¢××•×“×” 'Unnamed: 0' ××›×™×•×•×Ÿ ×©×”×™× ××™× ×“×§×¡
if 'Unnamed: 0' in df.columns:
    df = df.drop(columns=['Unnamed: 0'])

# Identify numerical columns for descriptive statistics
# Exclude columns that are likely identifiers, coordinates, or categorical
# ×–×™×”×•×™ ×¢××•×“×•×ª ××¡×¤×¨×™×•×ª ×œ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª
# ×”×—×¨×’ ×¢××•×“×•×ª ×©×¡×‘×™×¨ ×œ×”× ×™×— ×©×”×Ÿ ××–×”×™×, ×§×•××•×¨×“×™× ×˜×•×ª ××• ×§×˜×’×•×¨×™×•×ª
exclude_cols = ['TimeIndex', 'x_Pos', 'y_Pos', 'Parent', 'dt', 'ID', 'Experiment']

# Ensure 'FILENAME' is treated as a string for substring checks
# ×•×“× ×©-'FILENAME' ××˜×•×¤×œ ×›××—×¨×•×–×ª ×œ×‘×“×™×§×•×ª ×ª×ª-××—×¨×•×–×ª
if 'FILENAME' in df.columns:
    # Convert 'FILENAME' to lowercase 'filename' for consistency or specific handling
    df['filename'] = df['FILENAME'].astype(str) # Create the new column
    exclude_cols.append('filename') # Exclude the new column from numerical processing
    # Optionally, you might want to drop the original 'FILENAME' column if it's no longer needed
    # df = df.drop(columns=['FILENAME']) # Uncomment if you want to remove the original column
else:
    # If 'FILENAME' doesn't exist, check for 'filename' directly
    if 'filename' in df.columns:
        df['filename'] = df['filename'].astype(str)
        exclude_cols.append('filename')


numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
parameter_cols = [col for col in numerical_cols if col not in exclude_cols]

# --- Bar Graph of Parameter Means (Original) ---
# ×’×¨×£ ×¢××•×“×•×ª ×©×œ ×××•×¦×¢×™ ×¤×¨××˜×¨×™× (××§×•×¨×™)
# Calculate the mean of each parameter
# ×—×©×‘ ××ª ×××•×¦×¢ ×›×œ ×¤×¨××˜×¨
parameter_means = df[parameter_cols].mean().sort_values()

plt.figure(figsize=(15, 10)) # Increased figure size for better readability
parameter_means.plot(kind='bar')
plt.title('Mean Values of Parameters') # ×›×•×ª×¨×ª: '×¢×¨×›×™ ×××•×¦×¢ ×©×œ ×¤×¨××˜×¨×™×'
plt.ylabel('Mean Value') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×¢×¨×š ×××•×¦×¢'
plt.xlabel('Parameter') # ×ª×•×•×™×ª ×¦×™×¨ X: '×¤×¨××˜×¨'
plt.xticks(rotation=90, ha='right') # Rotate labels and align to right
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.savefig("parameter_means_bar_chart.png")
plt.close()

print("Bar chart of parameter means saved as parameter_means_bar_chart.png") # ×”×•×“×¢×”: "×’×¨×£ ×¢××•×“×•×ª ×©×œ ×××•×¦×¢×™ ×¤×¨××˜×¨×™× × ×©××¨ ×›-parameter_means_bar_chart.png"

# --- Distribution Plots (Histograms with KDE) for specific parameters ---
# ×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª (×”×™×¡×˜×•×’×¨××•×ª ×¢× KDE) ×¢×‘×•×¨ ×¤×¨××˜×¨×™× ×¡×¤×¦×™×¤×™×™×
# Define the specific parameters to visualize
# ×”×’×“×¨ ××ª ×”×¤×¨××˜×¨×™× ×”×¡×¤×¦×™×¤×™×™× ×œ×”×“××™×”
selected_params_for_dist = [
    'Area Shape',
    'TrackObjects_Displacement_50',
    'Acceleration',
    'TrackObjects_DistanceTraveled_50',
    'TrackObjects_Label_50',
    'TrackObjects_TrajectoryX_50',
    'speed_per_hour',
    'Distance'
]

# Filter to include only the selected parameters that exist in the DataFrame
# ×¡× ×Ÿ ×›×“×™ ×œ×›×œ×•×œ ×¨×§ ××ª ×”×¤×¨××˜×¨×™× ×©× ×‘×—×¨×• ×•×§×™×™××™× ×‘-DataFrame
selected_params_for_dist = [param for param in selected_params_for_dist if param in df.columns]

if selected_params_for_dist:
    # Create individual distribution plots for the selected parameters (Overall)
    # ×¦×•×¨ ×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×‘×•×“×“×™× ×¢×‘×•×¨ ×”×¤×¨××˜×¨×™× ×©× ×‘×—×¨×• (×¡×”"×›)
    if not os.path.exists("individual_distribution_plots_overall"):
        os.makedirs("individual_distribution_plots_overall")

    print("\nGenerating overall distribution plots...") # ×”×•×“×¢×”: "××™×™×¦×¨ ×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×›×œ×œ×™×™×..."
    for col in selected_params_for_dist:
        plt.figure(figsize=(8, 6))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Distribution of {col} (Overall)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×¡×”"×›)'
        plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
        plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
        plt.tight_layout()
        plt.savefig(f"individual_distribution_plots_overall/distribution_{col.replace('/', '_')}_overall.png")
        plt.close()
    print(f"Individual distribution plots for {len(selected_params_for_dist)} selected parameters saved in 'individual_distribution_plots_overall' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×‘×•×“×“×™× ×¢×‘×•×¨ {len(selected_params_for_dist)} ×¤×¨××˜×¨×™× × ×‘×—×¨×™× × ×©××¨×• ×‘×ª×™×§×™×™×ª 'individual_distribution_plots_overall'."

else:
    print("No selected parameters found to plot distributions.") # ×”×•×“×¢×”: "×œ× × ××¦××• ×¤×¨××˜×¨×™× × ×‘×—×¨×™× ×œ×”×¦×’×ª ×”×ª×¤×œ×’×•×™×•×ª."

# --- Separate and Overlap Distributions by Treatment Group ---
# ×”×¤×¨×“ ×•×”×¦×’ ×”×ª×¤×œ×’×•×™×•×ª ×—×•×¤×¤×•×ª ×œ×¤×™ ×§×‘×•×¦×ª ×˜×™×¤×•×œ
# Changed 'FILENAME' to 'filename' to match the user's request
if 'filename' in df.columns: # Check for the new lowercase column name
    # Create treatment groups based on 'B03' for Treatment 1 and 'B06' for Treatment 2 in filename
    # ×¦×•×¨ ×§×‘×•×¦×•×ª ×˜×™×¤×•×œ ×¢×œ ×‘×¡×™×¡ 'B03' ×œ×˜×™×¤×•×œ 1 ×•-'B06' ×œ×˜×™×¤×•×œ 2 ×‘×¢××•×“×ª filename
    df_treatment1 = df[df['filename'].str.contains('B03', na=False)].copy()
    df_treatment2 = df[df['filename'].str.contains('B06', na=False)].copy()

    print("\nProcessing data by treatment groups...") # ×”×•×“×¢×”: "××¢×‘×“ × ×ª×•× ×™× ×œ×¤×™ ×§×‘×•×¦×•×ª ×˜×™×¤×•×œ..."

    # Check if treatment dataframes are empty
    # ×‘×“×•×§ ×× ×”-DataFrames ×©×œ ×”×˜×™×¤×•×œ×™× ×¨×™×§×™×
    if not df_treatment1.empty:
        print(f"Found {len(df_treatment1)} rows for Treatment 1 (B03).") # ×”×•×“×¢×”: "× ××¦××• {len(df_treatment1)} ×©×•×¨×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 1 (B03)."
    else:
        print("No data found for Treatment 1 (B03).") # ×”×•×“×¢×”: "×œ× × ××¦××• × ×ª×•× ×™× ×¢×‘×•×¨ ×˜×™×¤×•×œ 1 (B03)."

    if not df_treatment2.empty:
        print(f"Found {len(df_treatment2)} rows for Treatment 2 (B06).") # ×”×•×“×¢×”: "× ××¦××• {len(df_treatment2)} ×©×•×¨×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 2 (B06)."
    else:
        print("No data found for Treatment 2 (B06).") # ×”×•×“×¢×”: "×œ× × ××¦××• × ×ª×•× ×™× ×¢×‘×•×¨ ×˜×™×¤×•×œ 2 (B06)."

    # Directories for saving plots
    # ×ª×™×§×™×•×ª ×œ×©××™×¨×ª ×’×¨×¤×™×
    output_dir_treatment1 = "distribution_plots_treatment1"
    output_dir_treatment2 = "distribution_plots_treatment2"
    output_dir_overlap = "distribution_plots_overlap"

    os.makedirs(output_dir_treatment1, exist_ok=True)
    os.makedirs(output_dir_treatment2, exist_ok=True)
    os.makedirs(output_dir_overlap, exist_ok=True)

    # Generate plots for each selected parameter
    # ×¦×•×¨ ×’×¨×¤×™× ×¢×‘×•×¨ ×›×œ ×¤×¨××˜×¨ × ×‘×—×¨
    for col in selected_params_for_dist:
        # Plot for Treatment 1
        # ×’×¨×£ ×¢×‘×•×¨ ×˜×™×¤×•×œ 1
        if not df_treatment1.empty:
            plt.figure(figsize=(8, 6))
            sns.histplot(df_treatment1[col], kde=True, bins=30, color='skyblue')
            plt.title(f'Distribution of {col} (Treatment 1 - B03)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×˜×™×¤×•×œ 1 - B03)'
            plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
            plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir_treatment1, f"distribution_{col.replace('/', '_')}_Treatment1.png"))
            plt.close()

        # Plot for Treatment 2
        # ×’×¨×£ ×¢×‘×•×¨ ×˜×™×¤×•×œ 2
        if not df_treatment2.empty:
            plt.figure(figsize=(8, 6))
            sns.histplot(df_treatment2[col], kde=True, bins=30, color='lightcoral')
            plt.title(f'Distribution of {col} (Treatment 2 - B06)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×˜×™×¤×•×œ 2 - B06)'
            plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
            plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir_treatment2, f"distribution_{col.replace('/', '_')}_Treatment2.png"))
            plt.close()

        # Overlap Plot
        # ×’×¨×£ ×—×•×¤×£
        if not df_treatment1.empty or not df_treatment2.empty:
            plt.figure(figsize=(10, 7))
            if not df_treatment1.empty:
                sns.histplot(df_treatment1[col], kde=True, bins=30, color='blue', label='Treatment 1 (B03)', alpha=0.6) # ×ª×•×•×™×ª: '×˜×™×¤×•×œ 1 (B03)'
            if not df_treatment2.empty:
                sns.histplot(df_treatment2[col], kde=True, bins=30, color='red', label='Treatment 2 (B06)', alpha=0.6) # ×ª×•×•×™×ª: '×˜×™×¤×•×œ 2 (B06)'
            plt.title(f'Distribution of {col} (Overlap)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×—×•×¤×£)'
            plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
            plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
            plt.legend()
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir_overlap, f"distribution_{col.replace('/', '_')}_Overlap.png"))
            plt.close()

    print(f"\nDistribution plots for Treatment 1 saved in '{output_dir_treatment1}' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 1 × ×©××¨×• ×‘×ª×™×§×™×™×ª '{output_dir_treatment1}'."
    print(f"Distribution plots for Treatment 2 saved in '{output_dir_treatment2}' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 2 × ×©××¨×• ×‘×ª×™×§×™×™×ª '{output_dir_treatment2}'."
    print(f"Overlaid distribution plots saved in '{output_dir_overlap}' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×—×•×¤×¤×™× × ×©××¨×• ×‘×ª×™×§×™×™×ª '{output_dir_overlap}'."

else:
    print("\n'filename' column not found in the DataFrame. Cannot separate by treatment groups.") # ×”×•×“×¢×”: "×¢××•×“×ª 'filename' ×œ× × ××¦××” ×‘-DataFrame. ×œ× × ×™×ª×Ÿ ×œ×”×¤×¨×™×“ ×œ×¤×™ ×§×‘×•×¦×•×ª ×˜×™×¤×•×œ."

# --- Descriptive Statistics Table (Original) ---
# ×˜×‘×œ×ª ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª (××§×•×¨×™)
# Compute descriptive statistics and save them to a CSV file
# ×—×©×‘ ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª ×•×©××•×¨ ××•×ª×” ×œ×§×•×‘×¥ CSV
descriptive_stats_table = df[parameter_cols].describe()
descriptive_stats_table.to_csv("descriptive_statistics_summary.csv")
print("\nDescriptive statistics summary table saved as descriptive_statistics_summary.csv") # ×”×•×“×¢×”: "×˜×‘×œ×ª ×¡×™×›×•× ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª × ×©××¨×” ×›-descriptive_statistics_summary.csv"

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
from scipy import stats # ×™×™×‘×•× ×¢×‘×•×¨ ×‘×“×™×§×•×ª ×¡×˜×˜×™×¡×˜×™×•×ª
from statsmodels.stats.multicomp import pairwise_tukeyhsd # ×™×™×‘×•× ×¢×‘×•×¨ ×‘×“×™×§×ª Tukey HSD

# Load the CSV file
# ×˜×¢×Ÿ ××ª ×§×•×‘×¥ ×”-CSV
file_path = '/content/Combined_SummaryTable_NoNormalization.csv'
df = pd.read_csv(file_path)

# Drop the 'Unnamed: 0' column as it's an index
# ×”×¡×¨ ××ª ×”×¢××•×“×” 'Unnamed: 0' ××›×™×•×•×Ÿ ×©×”×™× ××™× ×“×§×¡
if 'Unnamed: 0' in df.columns:
    df = df.drop(columns=['Unnamed: 0'])

# Identify numerical columns for descriptive statistics
# Exclude columns that are likely identifiers, coordinates, or categorical
# ×–×™×”×•×™ ×¢××•×“×•×ª ××¡×¤×¨×™×•×ª ×œ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª
# ×”×—×¨×’ ×¢××•×“×•×ª ×©×¡×‘×™×¨ ×œ×”× ×™×— ×©×”×Ÿ ××–×”×™×, ×§×•××•×¨×“×™× ×˜×•×ª ××• ×§×˜×’×•×¨×™×•×ª
exclude_cols = ['TimeIndex', 'x_Pos', 'y_Pos', 'Parent', 'dt', 'ID', 'Experiment']

# Ensure 'FILENAME' is treated as a string for substring checks
# ×•×“× ×©-'FILENAME' ××˜×•×¤×œ ×›××—×¨×•×–×ª ×œ×‘×“×™×§×•×ª ×ª×ª-××—×¨×•×–×ª
if 'FILENAME' in df.columns:
    # Convert 'FILENAME' to lowercase 'filename' for consistency or specific handling
    df['filename'] = df['FILENAME'].astype(str) # Create the new column
    exclude_cols.append('filename') # Exclude the new column from numerical processing
    # Optionally, you might want to drop the original 'FILENAME' column if it's no longer needed
    # df = df.drop(columns=['FILENAME']) # Uncomment if you want to remove the original column
else:
    # If 'FILENAME' doesn't exist, check for 'filename' directly
    if 'filename' in df.columns:
        df['filename'] = df['filename'].astype(str)
        exclude_cols.append('filename')


numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
parameter_cols = [col for col in numerical_cols if col not in exclude_cols]

# --- Bar Graph of Parameter Means (Original) ---
# ×’×¨×£ ×¢××•×“×•×ª ×©×œ ×××•×¦×¢×™ ×¤×¨××˜×¨×™× (××§×•×¨×™)
# Calculate the mean of each parameter
# ×—×©×‘ ××ª ×××•×¦×¢ ×›×œ ×¤×¨××˜×¨
parameter_means = df[parameter_cols].mean().sort_values()

plt.figure(figsize=(15, 10)) # Increased figure size for better readability
parameter_means.plot(kind='bar')
plt.title('Mean Values of Parameters') # ×›×•×ª×¨×ª: '×¢×¨×›×™ ×××•×¦×¢ ×©×œ ×¤×¨××˜×¨×™×'
plt.ylabel('Mean Value') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×¢×¨×š ×××•×¦×¢'
plt.xlabel('Parameter') # ×ª×•×•×™×ª ×¦×™×¨ X: '×¤×¨××˜×¨'
plt.xticks(rotation=90, ha='right') # Rotate labels and align to right
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.savefig("parameter_means_bar_chart.png")
plt.close()

print("Bar chart of parameter means saved as parameter_means_bar_chart.png") # ×”×•×“×¢×”: "×’×¨×£ ×¢××•×“×•×ª ×©×œ ×××•×¦×¢×™ ×¤×¨××˜×¨×™× × ×©××¨ ×›-parameter_means_bar_chart.png"

# --- Distribution Plots (Histograms with KDE) for specific parameters ---
# ×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª (×”×™×¡×˜×•×’×¨××•×ª ×¢× KDE) ×¢×‘×•×¨ ×¤×¨××˜×¨×™× ×¡×¤×¦×™×¤×™×™×
# Define the specific parameters to visualize
# ×”×’×“×¨ ××ª ×”×¤×¨××˜×¨×™× ×”×¡×¤×¦×™×¤×™×™× ×œ×”×“××™×”
selected_params_for_dist = [
    'Area Shape',
    'TrackObjects_Displacement_50',
    'Acceleration',
    'TrackObjects_DistanceTraveled_50',
    'TrackObjects_Label_50',
    'TrackObjects_TrajectoryX_50',
    'speed_per_hour',
    'Distance'
]

# Filter to include only the selected parameters that exist in the DataFrame
# ×¡× ×Ÿ ×›×“×™ ×œ×›×œ×•×œ ×¨×§ ××ª ×”×¤×¨××˜×¨×™× ×©× ×‘×—×¨×• ×•×§×™×™××™× ×‘-DataFrame
selected_params_for_dist = [param for param in selected_params_for_dist if param in df.columns]

if selected_params_for_dist:
    # Create individual distribution plots for the selected parameters (Overall)
    # ×¦×•×¨ ×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×‘×•×“×“×™× ×¢×‘×•×¨ ×”×¤×¨××˜×¨×™× ×©× ×‘×—×¨×• (×¡×”"×›)
    if not os.path.exists("individual_distribution_plots_overall"):
        os.makedirs("individual_distribution_plots_overall")

    print("\nGenerating overall distribution plots...") # ×”×•×“×¢×”: "××™×™×¦×¨ ×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×›×œ×œ×™×™×..."
    for col in selected_params_for_dist:
        plt.figure(figsize=(8, 6))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Distribution of {col} (Overall)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×¡×”"×›)'
        plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
        plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
        plt.tight_layout()
        plt.savefig(f"individual_distribution_plots_overall/distribution_{col.replace('/', '_')}_overall.png")
        plt.close()
    print(f"Individual distribution plots for {len(selected_params_for_dist)} selected parameters saved in 'individual_distribution_plots_overall' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×‘×•×“×“×™× ×¢×‘×•×¨ {len(selected_params_for_dist)} ×¤×¨××˜×¨×™× × ×‘×—×¨×™× × ×©××¨×• ×‘×ª×™×§×™×™×ª 'individual_distribution_plots_overall'."

else:
    print("No selected parameters found to plot distributions.") # ×”×•×“×¢×”: "×œ× × ××¦××• ×¤×¨××˜×¨×™× × ×‘×—×¨×™× ×œ×”×¦×’×ª ×”×ª×¤×œ×’×•×™×•×ª."

# --- Separate and Overlap Distributions by Treatment Group ---
# ×”×¤×¨×“ ×•×”×¦×’ ×”×ª×¤×œ×’×•×™×•×ª ×—×•×¤×¤×•×ª ×œ×¤×™ ×§×‘×•×¦×ª ×˜×™×¤×•×œ
if 'filename' in df.columns: # Check for the lowercase column name
    # Create treatment groups based on 'B03' for Treatment 1 and 'B06' for Treatment 2 in filename
    # ×¦×•×¨ ×§×‘×•×¦×•×ª ×˜×™×¤×•×œ ×¢×œ ×‘×¡×™×¡ 'B03' ×œ×˜×™×¤×•×œ 1 ×•-'B06' ×œ×˜×™×¤×•×œ 2 ×‘×¢××•×“×ª filename
    df_treatment1 = df[df['filename'].str.contains('B03', na=False)].copy()
    df_treatment2 = df[df['filename'].str.contains('B06', na=False)].copy()

    print("\nProcessing data by treatment groups...") # ×”×•×“×¢×”: "××¢×‘×“ × ×ª×•× ×™× ×œ×¤×™ ×§×‘×•×¦×•×ª ×˜×™×¤×•×œ..."

    # Check if treatment dataframes are empty
    # ×‘×“×•×§ ×× ×”-DataFrames ×©×œ ×”×˜×™×¤×•×œ×™× ×¨×™×§×™×
    if not df_treatment1.empty:
        print(f"Found {len(df_treatment1)} rows for Treatment 1 (B03).") # ×”×•×“×¢×”: "× ××¦××• {len(df_treatment1)} ×©×•×¨×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 1 (B03)."
    else:
        print("No data found for Treatment 1 (B03).") # ×”×•×“×¢×”: "×œ× × ××¦××• × ×ª×•× ×™× ×¢×‘×•×¨ ×˜×™×¤×•×œ 1 (B03)."

    if not df_treatment2.empty:
        print(f"Found {len(df_treatment2)} rows for Treatment 2 (B06).") # ×”×•×“×¢×”: "× ××¦××• {len(df_treatment2)} ×©×•×¨×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 2 (B06)."
    else:
        print("No data found for Treatment 2 (B06).") # ×”×•×“×¢×”: "×œ× × ××¦××• × ×ª×•× ×™× ×¢×‘×•×¨ ×˜×™×¤×•×œ 2 (B06)."

    # Directories for saving plots
    # ×ª×™×§×™×•×ª ×œ×©××™×¨×ª ×’×¨×¤×™×
    output_dir_treatment1 = "distribution_plots_treatment1"
    output_dir_treatment2 = "distribution_plots_treatment2"
    output_dir_overlap = "distribution_plots_overlap"

    os.makedirs(output_dir_treatment1, exist_ok=True)
    os.makedirs(output_dir_treatment2, exist_ok=True)
    os.makedirs(output_dir_overlap, exist_ok=True)

    # Generate plots for each selected parameter
    # ×¦×•×¨ ×’×¨×¤×™× ×¢×‘×•×¨ ×›×œ ×¤×¨××˜×¨ × ×‘×—×¨
    for col in selected_params_for_dist:
        # Plot for Treatment 1
        # ×’×¨×£ ×¢×‘×•×¨ ×˜×™×¤×•×œ 1
        if not df_treatment1.empty:
            plt.figure(figsize=(8, 6))
            sns.histplot(df_treatment1[col], kde=True, bins=30, color='skyblue')
            plt.title(f'Distribution of {col} (Treatment 1 - B03)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×˜×™×¤×•×œ 1 - B03)'
            plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
            plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir_treatment1, f"distribution_{col.replace('/', '_')}_Treatment1.png"))
            plt.close()

        # Plot for Treatment 2
        # ×’×¨×£ ×¢×‘×•×¨ ×˜×™×¤×•×œ 2
        if not df_treatment2.empty:
            plt.figure(figsize=(8, 6))
            sns.histplot(df_treatment2[col], kde=True, bins=30, color='lightcoral')
            plt.title(f'Distribution of {col} (Treatment 2 - B06)') # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×˜×™×¤×•×œ 2 - B06)'
            plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
            plt.ylabel('Frequency') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×ª×“×™×¨×•×ª'
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir_treatment2, f"distribution_{col.replace('/', '_')}_Treatment2.png"))
            plt.close()

        # Overlap Plot with Tukey ANOVA P-value
        # ×’×¨×£ ×—×•×¤×£ ×¢× ×¢×¨×š P ×©×œ Tukey ANOVA
        if not df_treatment1.empty or not df_treatment2.empty:
            plt.figure(figsize=(10, 7))

            # Perform Tukey HSD only if both treatment groups have data for the current column
            # ×‘×¦×¢ Tukey HSD ×¨×§ ×× ×œ×©×ª×™ ×§×‘×•×¦×•×ª ×”×˜×™×¤×•×œ ×™×© × ×ª×•× ×™× ×¢×‘×•×¨ ×”×¢××•×“×” ×”× ×•×›×—×™×ª
            tukey_p_value = np.nan
            combined_data = []
            group_labels = []

            # Collect data for Tukey HSD
            # ××¡×•×£ × ×ª×•× ×™× ×¢×‘×•×¨ Tukey HSD
            if not df_treatment1.empty and col in df_treatment1.columns:
                combined_data.extend(df_treatment1[col].dropna().tolist())
                group_labels.extend(['B03'] * len(df_treatment1[col].dropna()))
            if not df_treatment2.empty and col in df_treatment2.columns:
                combined_data.extend(df_treatment2[col].dropna().tolist())
                group_labels.extend(['B06'] * len(df_treatment2[col].dropna()))

            # Ensure there's enough data and at least two groups for Tukey HSD
            # ×•×“× ×©×™×© ××¡×¤×™×§ × ×ª×•× ×™× ×•×œ×¤×—×•×ª ×©×ª×™ ×§×‘×•×¦×•×ª ×¢×‘×•×¨ Tukey HSD
            if len(set(group_labels)) > 1 and len(combined_data) > 0:
                try:
                    tukey_result = pairwise_tukeyhsd(endog=combined_data, groups=group_labels, alpha=0.05)
                    # Find the p-value for the B03 vs B06 comparison
                    # ××¦× ××ª ×¢×¨×š ×”-P ×œ×”×©×•×•××” ×‘×™×Ÿ B03 ×œ-B06
                    for row in tukey_result._results_table.data[1:]: # Skip header row
                        if (row[0] == 'B03' and row[1] == 'B06') or (row[0] == 'B06' and row[1] == 'B03'):
                            tukey_p_value = row[5] # p-value is at index 5
                            break
                except Exception as e:
                    print(f"Could not perform Tukey HSD for {col}: {e}")
            else:
                print(f"Not enough data or groups to perform Tukey HSD for {col}.")


            if not df_treatment1.empty:
                sns.histplot(df_treatment1[col], kde=True, bins=30, color='blue', label='Treatment 1 (B03)', alpha=0.6, stat='density') # ×©×™× ×•×™ ×œ-stat='density'
            if not df_treatment2.empty:
                sns.histplot(df_treatment2[col], kde=True, bins=30, color='red', label='Treatment 2 (B06)', alpha=0.6, stat='density') # ×©×™× ×•×™ ×œ-stat='density'

            title_text = f'Distribution of {col} (Overlap)'
            if not np.isnan(tukey_p_value):
                title_text += f'\nTukey HSD p-value: {tukey_p_value:.4f}'
            else:
                title_text += '\nTukey HSD p-value: N/A'

            plt.title(title_text) # ×›×•×ª×¨×ª: '×”×ª×¤×œ×’×•×ª ×©×œ {col} (×—×•×¤×£) \n ×¢×¨×š P ×©×œ Tukey HSD: {tukey_p_value:.4f}'
            plt.xlabel(col) # ×ª×•×•×™×ª ×¦×™×¨ X: {col}
            plt.ylabel('Density') # ×ª×•×•×™×ª ×¦×™×¨ Y: '×¦×¤×™×¤×•×ª' (×›××©×¨ stat='density')
            plt.legend()
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir_overlap, f"distribution_{col.replace('/', '_')}_Overlap.png"))
            plt.close()

    print(f"\nDistribution plots for Treatment 1 saved in '{output_dir_treatment1}' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 1 × ×©××¨×• ×‘×ª×™×§×™×™×ª '{output_dir_treatment1}'."
    print(f"Distribution plots for Treatment 2 saved in '{output_dir_treatment2}' directory.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ 2 × ×©××¨×• ×‘×ª×™×§×™×™×ª '{output_dir_treatment2}'."
    print(f"Overlaid distribution plots saved in '{output_dir_overlap}' directory, including Tukey HSD p-values.") # ×”×•×“×¢×”: "×’×¨×¤×™ ×”×ª×¤×œ×’×•×ª ×—×•×¤×¤×™× × ×©××¨×• ×‘×ª×™×§×™×™×ª '{output_dir_overlap}', ×›×•×œ×œ ×¢×¨×›×™ P ×©×œ Tukey HSD."

else:
    print("\n'filename' column not found in the DataFrame. Cannot separate by treatment groups.") # ×”×•×“×¢×”: "×¢××•×“×ª 'filename' ×œ× × ××¦××” ×‘-DataFrame. ×œ× × ×™×ª×Ÿ ×œ×”×¤×¨×™×“ ×œ×¤×™ ×§×‘×•×¦×•×ª ×˜×™×¤×•×œ."

# --- Descriptive Statistics Table (Original) ---
# ×˜×‘×œ×ª ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª (××§×•×¨×™)
# Compute descriptive statistics and save them to a CSV file
# ×—×©×‘ ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª ×•×©××•×¨ ××•×ª×” ×œ×§×•×‘×¥ CSV
descriptive_stats_table = df[parameter_cols].describe()
descriptive_stats_table.to_csv("descriptive_statistics_summary.csv")
print("\nDescriptive statistics summary table saved as descriptive_statistics_summary.csv") # ×”×•×“×¢×”: "×˜×‘×œ×ª ×¡×™×›×•× ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª × ×©××¨×” ×›-descriptive_statistics_summary.csv"

"""# **Distribution Map - Feature_Time - Heat Map Per Treatment**"""

# ğŸ“¦ Install libraries (if needed)
!pip install openpyxl --quiet

# ğŸ“š Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“„ Specify the path to the file
# Copy the path from the File tab in Google Colab
# Ensure this file contains the non-normalized data you wish to display.
input_file_path = '/content/Combined_SummaryTable_NoNormalization.csv'

# ğŸ“¥ Read the file
try:
    df = pd.read_csv(input_file_path)
    print("DataFrame loaded successfully. First 5 rows:")
    print(df.head())
except FileNotFoundError:
    print(f"Error: The file '{input_file_path}' was not found.")
    print("Please ensure the file is uploaded to Google Colab and the path is correct.")
    exit()
except Exception as e:
    print(f"An error occurred while loading the file: {e}")
    exit()

# ğŸ§ª Map treatments from filename
def extract_treatment(name):
    # Ensure conversion to string in case filename is not already a string
    if 'B03' in str(name):
        return 'B03'
    elif 'B06' in str(name):
        return 'B06'
    else:
        return 'Other'

df['treatment'] = df['filename'].apply(extract_treatment)

# ğŸ¨ Function to plot feature distribution heatmap over time
def plot_distribution_heatmap(sub_df, display_treatment_label, feature_col_name, time_col='image_number', bin_count=50):
    # Ensure the column exists before attempting to access it
    if feature_col_name not in sub_df.columns:
        print(f"Warning: Column '{feature_col_name}' not found for treatment {display_treatment_label}. Skipping.")
        return

    # Remove rows with NaN values in the feature column or time column
    cleaned_sub_df = sub_df.dropna(subset=[feature_col_name, time_col])

    if cleaned_sub_df.empty:
        print(f"No sufficient data for feature '{feature_col_name}' and treatment '{display_treatment_label}' after NaN removal. Skipping.")
        return

    feature_min = cleaned_sub_df[feature_col_name].min()
    feature_max = cleaned_sub_df[feature_col_name].max()

    # Handle cases where feature values are constant or too few for meaningful bins
    if feature_min == feature_max or cleaned_sub_df[feature_col_name].nunique() < 2:
        print(f"Warning: Feature '{feature_col_name}' in treatment '{display_treatment_label}' has constant or too few unique values. Heatmap might not be informative.")
        if feature_min == feature_max:
             bins = np.array([feature_min - 0.1, feature_min, feature_max + 0.1])
        else:
             bins = np.linspace(feature_min, feature_max, 3)
        if len(bins) <= 1:
            bins = np.array([feature_min - 1, feature_max + 1])
    else:
        bins = np.linspace(feature_min, feature_max, bin_count + 1)


    timepoints = sorted(cleaned_sub_df[time_col].unique())

    # Skip if no timepoints available
    if not timepoints:
        print(f"No timepoints available for feature '{feature_col_name}' and treatment '{display_treatment_label}'. Skipping.")
        return

    heatmap_matrix = np.zeros((len(bins) - 1, len(timepoints)))

    for t_idx, t in enumerate(timepoints):
        values = cleaned_sub_df[cleaned_sub_df[time_col] == t][feature_col_name].dropna()
        if not values.empty:
            hist, _ = np.histogram(values, bins=bins)
            heatmap_matrix[:, t_idx] = hist

    # Plot the heatmap
    plt.figure(figsize=(14, 6))
    sns.heatmap(heatmap_matrix, cmap='viridis', cbar_kws={'label': 'Cell Count'})
    plt.xlabel('Time (Hours)')
    plt.ylabel(f'{feature_col_name} bins')
    # Use the custom display label in the title
    plt.title(f'Distribution of {feature_col_name} Over Time â€” {display_treatment_label}')

    # Configure x-axis ticks for readability
    if len(timepoints) > 0:
        if len(timepoints) < 20:
            plt.xticks(ticks=np.arange(len(timepoints)), labels=timepoints, rotation=90)
        else:
            plt.xticks(ticks=np.arange(0, len(timepoints), max(1, len(timepoints)//10)),
                       labels=timepoints[::max(1, len(timepoints)//10)], rotation=90)

    # Configure y-axis ticks for readability
    num_yticks = 10
    if len(bins) - 1 > 0:
        if len(bins) - 1 > num_yticks:
            step = max(1, (len(bins) - 1) // num_yticks)
            ytick_positions = np.arange(0, len(bins) -1, step)
            ytick_labels = [f'{bins[i]:.2f}' for i in ytick_positions]
            plt.yticks(ticks=ytick_positions, labels=ytick_labels, rotation=0)
        else:
            plt.yticks(ticks=np.arange(len(bins)-1), labels=[f'{b:.2f}' for b in bins[:-1]], rotation=0)

    plt.tight_layout()
    plt.show()

# --- List of features to plot ---
# These column names must EXACTLY match the column names in your CSV file
features_to_plot = [
    'Area Shape',
    'Acceleration',
    'speed_per_hour',
    'Distance'
]

# âœ… Check for required columns (updated for all features)
expected_cols_all = ['filename', 'image_number'] + features_to_plot
missing_cols_all = [col for col in expected_cols_all if col not in df.columns]
if missing_cols_all:
    print("\n--- ERROR: MISSING COLUMNS DETECTED ---")
    print(f"The following required columns were NOT found in your CSV file: {missing_cols_all}")
    print("Please verify that these column names exactly match the headers in your 'Combined_SummaryTable_NoNormalization.csv' file.")
    print("If the names are different, please update the 'features_to_plot' list in the code.")
    print("------------------------------------------")
    raise ValueError(f"Missing required columns in your CSV file: {missing_cols_all}. Please verify column names.")


# --- Generate heatmaps for each feature and each treatment ---
# Mapping for display labels
display_treatment_map = {
    'B03': 'Treatment 1',
    'B06': 'Treatment 2'
}

for feature in features_to_plot:
    for treatment_code in ['B03', 'B06']:
        # Get the display label for the current treatment
        display_label = display_treatment_map.get(treatment_code, treatment_code) # Fallback to code if not in map

        sub_df = df[df['treatment'] == treatment_code].copy()
        if sub_df.empty:
            print(f"No data found for {display_label} (Code: '{treatment_code}') and feature '{feature}'. Skipping plot generation.")
        else:
            print(f"\nGenerating heatmap for: Feature '{feature}', {display_label} (Code: '{treatment_code}')...")
            # Pass the display_label to the plotting function
            plot_distribution_heatmap(sub_df, display_label, feature)

print("\nAll heatmaps generated successfully.")



"""# **Distribution Map - Feature_Time - Heat Map - Not Per Treatment **"""

# ğŸ“¦ ×”×ª×§× ×ª ×¡×¤×¨×™×•×ª (×× ×¦×¨×™×š)
!pip install openpyxl --quiet

# ğŸ“š ×™×™×‘×•× ×¡×¤×¨×™×•×ª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“„ ×¦×™×•×Ÿ ×”× ×ª×™×‘ ×œ×§×•×‘×¥ (×œ×”×¢×ª×™×§ ×-Files ×©×œ ×’×•×’×œ ×§×•×œ××‘)
#input_file_path = '/content/Combined_SummaryTable_Processed .csv'  # <- ×”×“×‘×™×§×™ ×›××Ÿ ××ª ×”× ×ª×™×‘ ×©×œ×š
input_file_path = '/content/Combined_SummaryTable_NoNormalization.csv'
# ğŸ“¥ ×§×¨×™××ª ×”×§×•×‘×¥
df = pd.read_csv(input_file_path)

# âœ… ×‘×“×™×§×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª
expected_cols = ['Area Shape', 'image_number']
missing_cols = [col for col in expected_cols if col not in df.columns]
if missing_cols:
    raise ValueError(f"Missing required columns: {missing_cols}")

# ğŸ¨ ×¤×•× ×§×¦×™×” ×œ×¦×™×•×¨ ××¤×ª ×—×•× ×©×œ ×”×ª×¤×œ×’×•×ª ×”×¤×™×¦'×¨ ×‘×–××Ÿ
def plot_distribution_heatmap(df, feature_col='Area Shape', time_col='image_number', bin_count=50):
    feature_min = df[feature_col].min()
    feature_max = df[feature_col].max()
    bins = np.linspace(feature_min, feature_max, bin_count + 1)

    timepoints = sorted(df[time_col].dropna().unique())
    heatmap_matrix = np.zeros((bin_count, len(timepoints)))

    for t_idx, t in enumerate(timepoints):
        values = df[df[time_col] == t][feature_col].dropna()
        hist, _ = np.histogram(values, bins=bins)
        heatmap_matrix[:, t_idx] = hist

    # ×¦×™×•×¨ ××¤×ª ×—×•×
    plt.figure(figsize=(14, 6))
    sns.heatmap(heatmap_matrix, cmap='viridis', cbar_kws={'label': 'Cell Count'})
    plt.xlabel('Time (image_number)')
    plt.ylabel(f'{feature_col} bins')
    plt.title(f'Distribution of {feature_col} Over Time â€” All Treatments')
    plt.xticks(ticks=np.arange(len(timepoints)), labels=timepoints, rotation=90)
    plt.tight_layout()
    plt.show()

# ğŸ–¼ ×”×¤×¢×œ×ª ×”×¤×•× ×§×¦×™×” ×¢×‘×•×¨ ×›×œ ×”×“××˜×”
plot_distribution_heatmap(df)

"""# **Statistics**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- ×”×•×¨××•×ª ×œ×”×¢×œ××ª ×§×•×‘×¥ ×•× ×ª×™×‘ ---
# 1. ×”×¢×œ×” ××ª ×§×•×‘×¥ ×”-CSV ×©×œ×š ('all_forecast_metrics_summary_table_1.csv')
#    ×œ×“×¤×“×¤×Ÿ ×”×§×‘×¦×™× ×©×œ Google Colab (××™×™×§×•×Ÿ ×”×ª×™×§×™×” ×‘×¦×“ ×©×××œ).
# 2. ×œ××—×¨ ×”×”×¢×œ××”, ×œ×—×¥/×™ ×§×œ×™×§ ×™×× ×™ ×¢×œ ×©× ×”×§×•×‘×¥ ×‘×“×¤×“×¤×Ÿ ×”×§×‘×¦×™× ×©×œ Colab
#    ×•×‘×—×¨/×™ "Copy path".
# 3. ×”×“×‘×§/×™ ××ª ×”× ×ª×™×‘ ×©×”×¢×ª×§×ª ×œ×ª×•×š ×”××©×ª× ×” 'input_file_path' ××˜×”.
#    ×”×•× ×××•×¨ ×œ×”×™×¨××•×ª ×‘×¢×¨×š ×›×š: '/content/all_forecast_metrics_summary_table_1.csv'

input_file_path = '/content/all_forecast_metrics_summary_table (8).csv'  # <--- ×”×“×‘×§/×™ ×›××Ÿ ××ª ×”× ×ª×™×‘ ×©×”×¢×ª×§×ª

# --- ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ---
try:
    df_raw = pd.read_csv(input_file_path)
    print("DataFrame × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”. 5 ×”×©×•×¨×•×ª ×”×¨××©×•× ×•×ª:")
    print(df_raw.head())
except FileNotFoundError:
    print(f"×©×’×™××”: ×”×§×•×‘×¥ '{input_file_path}' ×œ× × ××¦×.")
    print("×× × ×•×“×/×™ ×©×”×§×•×‘×¥ ×”×•×¢×œ×” ×œ-Colab ×•×©× ×ª×™×‘ ×”×§×•×‘×¥ × ×›×•×Ÿ.")
    exit()
except Exception as e:
    print(f"××™×¨×¢×” ×©×’×™××” ×‘×¢×ª ×˜×¢×™× ×ª ×”×§×•×‘×¥: {e}")
    exit()

# --- ×‘×“×™×§×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª ×‘×˜×‘×œ×ª ×”×§×œ×˜ ×”×’×•×œ××™×ª ---
expected_raw_cols = ['Treatment_Alias', 'Feature', 'MAPE', 'P-Value', 'is_good_forecast']
missing_raw_cols = [col for col in expected_raw_cols if col not in df_raw.columns]
if missing_raw_cols:
    raise ValueError(f"×©×’×™××”: ×¢××•×“×•×ª × ×“×¨×©×•×ª ×—×¡×¨×•×ª ×‘×§×•×‘×¥ ×”-CSV ×©×œ×š: {missing_raw_cols}. ×× × ×•×“×/×™ ××ª ×©××•×ª ×”×¢××•×“×•×ª.")

# --- ×”××¨×ª 'is_good_forecast' ×œ-boolean ---
df_raw['is_good_forecast'] = df_raw['is_good_forecast'].apply(lambda x: str(x).lower() == 'true')

# --- ×”×’×“×¨×ª ×§×¨×™×˜×¨×™×•× ×™× ×œ-"Good" ×¢×‘×•×¨ MAPE ×•-P-Value ×‘×œ×‘×“ ---
def calculate_forecast_quality(row):
    results = {}

    # --- MAPE based criterion (using is_good_forecast column) ---
    # The 'is_good_forecast' column is defined as MAPE <= 10%
    results['MAPE'] = 'Good Forecast (MAPE â‰¤ 10%)' if row['is_good_forecast'] else 'Bad Forecast (MAPE > 10%)'

    # --- P-Value based criterion (P-Value <= 0.05) ---
    results['P-Value'] = 'Good Forecast (P â‰¤ 0.05)' if row['P-Value'] <= 0.05 else 'Bad Forecast (P > 0.05)'

    return results

# ×™×¦×™×¨×ª DataFrame ×—×“×© ×©×™×›×™×œ ××ª ×¡×˜×˜×•×¡ ×”-Good/Bad ×œ×›×œ ×©×™×œ×•×‘ ×©×œ ×˜×™×¤×•×œ, ×¤×™×¦'×¨ ×•××“×“
melted_forecast_quality = []

for index, row in df_raw.iterrows():
    quality_results = calculate_forecast_quality(row)
    for metric_name, quality_status in quality_results.items():
        melted_forecast_quality.append({
            'Treatment_Alias': row['Treatment_Alias'],
            'Feature': row['Feature'],
            'Metric': metric_name,
            'Forecast_Quality': quality_status,
            'Value': 1 # Dummy value for counting, we'll sum these up
        })

df_melted_quality = pd.DataFrame(melted_forecast_quality)

# --- ×¡×™×›×•× × ×ª×•× ×™× ×œ××—×•×–×™× ---
# × ×¡×›× ××ª ×¡×¤×™×¨×ª ×”×ª××™× (×©×•×¨×•×ª) ×œ×›×œ ×§×‘×•×¦×” ×©×œ ×˜×™×¤×•×œ, ×¤×™×¦'×¨, ××“×“ ×•××™×›×•×ª ×—×™×–×•×™
summary_counts = df_melted_quality.groupby(['Treatment_Alias', 'Feature', 'Metric', 'Forecast_Quality']).size().reset_index(name='Count')

# × ×—×©×‘ ××ª ×¡×š ×”×ª××™× ×œ×›×œ ×˜×™×¤×•×œ, ×¤×™×¦'×¨ ×•××“×“ (×›×“×™ ×©× ×•×›×œ ×œ×—×©×‘ ××—×•×–×™×)
total_counts_per_group = summary_counts.groupby(['Treatment_Alias', 'Feature', 'Metric'])['Count'].transform('sum').reset_index(name='Total')
summary_counts['Total'] = total_counts_per_group['Total'] # Add Total column back

# × ×—×©×‘ ××ª ×”××—×•×–×™×
summary_counts['Percentage'] = (summary_counts['Count'] / summary_counts['Total']) * 100

# --- ×”×›× ×ª × ×ª×•× ×™× ×œ×’×¨×£ ---
# × ×“××’ ×©×™×”×™×• ×œ× ×• ×’× ×©×•×¨×•×ª ×¢×‘×•×¨ "Good" ×•×’× ×¢×‘×•×¨ "Bad" ×œ×›×œ ×§×‘×•×¦×”, ×’× ×× Count ×”×•× 0
# × ×‘× ×” DataFrame ×©×œ ×›×œ ×”×©×™×œ×•×‘×™× ×”××¤×©×¨×™×™× ×¢×‘×•×¨ MAPE ×•-P-Value ×‘×œ×‘×“
all_combinations = pd.MultiIndex.from_product(
    [df_raw['Treatment_Alias'].unique(),
     df_raw['Feature'].unique(),
     ['MAPE', 'P-Value'], # ×¨×§ MAPE ×•-P-Value
     ['Good Forecast (MAPE â‰¤ 10%)', 'Bad Forecast (MAPE > 10%)',
      'Good Forecast (P â‰¤ 0.05)', 'Bad Forecast (P > 0.05)']],
    names=['Treatment_Alias', 'Feature', 'Metric', 'Forecast_Quality']
).to_frame(index=False)

# × ××–×’ ×¢× ×”× ×ª×•× ×™× ×©×—×™×©×‘× ×•
df_final_plot = pd.merge(all_combinations, summary_counts,
                         on=['Treatment_Alias', 'Feature', 'Metric', 'Forecast_Quality'],
                         how='left').fillna(0)

# × × ×§×” ×¢×¨×›×™× ×©×œ× ×¨×œ×•×•× ×˜×™×™× (×›××• ×©×™×œ×•×‘×™× ×©×œ ×ª×•×•×™×•×ª Good/Bad ×©×’×•×™×•×ª ×¢× ××“×“×™×)
df_final_plot = df_final_plot[df_final_plot['Percentage'] > 0] # × ×¡× ×Ÿ ×©×•×¨×•×ª ×¨×™×§×•×ª

# × ×¡×“×¨ ××—×“×© ××ª ×”× ×ª×•× ×™× ×œ×¤×•×¨××˜ ×”××ª××™× ×œ×’×¨×£
df_final_plot['Forecast_Type'] = df_final_plot['Forecast_Quality'].apply(lambda x: 'Good Forecast' if 'Good' in x else 'Bad Forecast')
df_final_plot['Forecast_Label'] = df_final_plot['Forecast_Quality'] # Keep original label for legend

# × ×¡×›× ××ª ×”× ×ª×•× ×™× ×œ×¤×™ ×˜×™×¤×•×œ, ×¤×™×¦'×¨, ××“×“ ×•×¡×•×’ ×—×™×–×•×™
df_pivot = df_final_plot.pivot_table(index=['Treatment_Alias', 'Feature', 'Metric'],
                                     columns='Forecast_Type',
                                     values='Percentage',
                                     fill_value=0).reset_index()

df_pivot.columns.name = None
df_pivot = df_pivot.rename(columns={'Good Forecast': 'Good_Percentage', 'Bad Forecast': 'Bad_Percentage'})

df_plot_final_melted = df_pivot.melt(id_vars=['Treatment_Alias', 'Feature', 'Metric'],
                                     value_vars=['Good_Percentage', 'Bad_Percentage'],
                                     var_name='Forecast_Quality_Type',
                                     value_name='Percentage')

df_plot_final_melted['Forecast_Quality_Type'] = df_plot_final_melted['Forecast_Quality_Type'].replace({
    'Good_Percentage': 'Good Forecast',
    'Bad_Percentage': 'Bad Forecast'
})

# --- ×™×¦×™×¨×ª ×’×¨×£ ×”×¢××•×“×•×ª ×”××•×¢×¨× ---
# × ×’×“×™×¨ ××ª ×¡×“×¨ ×”××“×“×™× ×œ×”×¦×’×” (×¨×§ MAPE ×•-P-Value)
metric_order = ['P-Value']
feature_order = df_plot_final_melted['Feature'].unique()

g = sns.FacetGrid(df_plot_final_melted, col="Treatment_Alias", row="Metric",
                  height=4, aspect=1.5, sharey=True,
                  col_order=df_plot_final_melted['Treatment_Alias'].unique(),
                  row_order=metric_order) # ×¡×“×¨ ×”×©×•×¨×•×ª (××“×“×™×) - ×¨×§ MAPE ×•-P-Value


g.map_dataframe(sns.barplot, x='Feature', y='Percentage', hue='Forecast_Quality_Type',
                palette={'Good Forecast': 'lightgreen', 'Bad Forecast': 'lightcoral'},
                order=feature_order, # ×¡×“×¨ ×”×¤×™×¦'×¨×™× ×‘×ª×•×š ×›×œ ×’×¨×£
                estimator=sum, errorbar=None)
g.add_legend(title="Forecast Quality")

g.set_axis_labels("Feature", "Percentage (%)")
g.set_titles(col_template="Treatment: {col_name}", row_template="Metric: {row_name}")


#g.add_legend(title="Forecast Quality")

plt.suptitle('Percentage of Good vs. Bad Forecasts for P-Value per Feature and Treatment', y=1.02, fontsize=16)

plt.tight_layout(rect=[0, 0, 1, 0.98])

plt.show()

"""×¡×˜×˜×™×¡×˜×™×§×” ×¢× ×¡×˜×˜×™×¡×˜×™×§×”"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
from statsmodels.stats.multitest import multipletests

# --- ×”×•×¨××•×ª ×œ×”×¢×œ××ª ×§×•×‘×¥ ×•× ×ª×™×‘ ---
# 1. ×”×¢×œ×” ××ª ×§×•×‘×¥ ×”-CSV ×©×œ×š ('all_forecast_metrics_summary_table_1.csv')
# Â  Â ×œ×“×¤×“×¤×Ÿ ×”×§×‘×¦×™× ×©×œ Google Colab (××™×™×§×•×Ÿ ×”×ª×™×§×™×” ×‘×¦×“ ×©×××œ).
# 2. ×œ××—×¨ ×”×”×¢×œ××”, ×œ×—×¥/×™ ×§×œ×™×§ ×™×× ×™ ×¢×œ ×©× ×”×§×•×‘×¥ ×‘×“×¤×“×¤×Ÿ ×”×§×‘×¦×™× ×©×œ Colab
# Â  Â ×•×‘×—×¨/×™ "Copy path".
# 3. ×”×“×‘×§/×™ ××ª ×”× ×ª×™×‘ ×©×”×¢×ª×§×ª ×œ×ª×•×š ×”××©×ª× ×” 'input_file_path' ××˜×”.
# Â  Â ×”×•× ×××•×¨ ×œ×”×™×¨××•×ª ×‘×¢×¨×š ×›×š: '/content/all_forecast_metrics_summary_table_1.csv'

input_file_path = '/content/all_forecast_metrics_summary_table (8).csv' # <--- ×”×“×‘×§/×™ ×›××Ÿ ××ª ×”× ×ª×™×‘ ×©×”×¢×ª×§×ª

# --- ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ---
try:
    df_raw = pd.read_csv(input_file_path)
    print("DataFrame loaded successfully. First 5 rows:")
    print(df_raw.head())
except FileNotFoundError:
    print(f"Error: File '{input_file_path}' not found.")
    print("Please ensure the file is uploaded to Colab and the file path is correct.")
    exit()
except Exception as e:
    print(f"An error occurred while loading the file: {e}")
    exit()

# --- ×‘×“×™×§×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª ×‘×˜×‘×œ×ª ×”×§×œ×˜ ×”×’×•×œ××™×ª ---
expected_raw_cols = ['Treatment_Alias', 'Feature', 'MAPE', 'P-Value', 'is_good_forecast']
missing_raw_cols = [col for col in expected_raw_cols if col not in df_raw.columns]
if missing_raw_cols:
    raise ValueError(f"Error: Required columns are missing from your CSV file: {missing_raw_cols}. Please verify column names.")

# --- ×”××¨×ª 'is_good_forecast' ×œ-boolean ---
df_raw['is_good_forecast'] = df_raw['is_good_forecast'].apply(lambda x: str(x).lower() == 'true')

# --- ×”×’×“×¨×ª ×§×¨×™×˜×¨×™×•× ×™× ×œ-"Good" ×¢×‘×•×¨ MAPE ×•-P-Value ×‘×œ×‘×“ ---
def calculate_forecast_quality(row):
    results = {}

    # --- MAPE based criterion (using is_good_forecast column) ---
    # The 'is_good_forecast' column is defined as MAPE <= 10%
    results['MAPE'] = 'Good Forecast (MAPE â‰¤ 10%)' if row['is_good_forecast'] else 'Bad Forecast (MAPE > 10%)'

    # --- P-Value based criterion (P-Value <= 0.05) ---
    results['P-Value'] = 'Good Forecast (P â‰¤ 0.05)' if row['P-Value'] <= 0.05 else 'Bad Forecast (P > 0.05)'

    return results

# ×™×¦×™×¨×ª DataFrame ×—×“×© ×©×™×›×™×œ ××ª ×¡×˜×˜×•×¡ ×”-Good/Bad ×œ×›×œ ×©×™×œ×•×‘ ×©×œ ×˜×™×¤×•×œ, ×¤×™×¦'×¨ ×•××“×“
melted_forecast_quality = []

for index, row in df_raw.iterrows():
    quality_results = calculate_forecast_quality(row)
    for metric_name, quality_status in quality_results.items():
        melted_forecast_quality.append({
            'treatment': row['Treatment_Alias'], # Changed from Treatment_Alias to treatment
            'Feature': row['Feature'],
            'Metric': metric_name,
            'Forecast_Quality': quality_status,
            'Value': 1 # Dummy value for counting, we'll sum these up
        })

df_melted_quality = pd.DataFrame(melted_forecast_quality)

# --- ×¡×™×›×•× × ×ª×•× ×™× ×œ××—×•×–×™× ---
# × ×¡×›× ××ª ×¡×¤×™×¨×ª ×”×ª××™× (×©×•×¨×•×ª) ×œ×›×œ ×§×‘×•×¦×” ×©×œ ×˜×™×¤×•×œ, ×¤×™×¦'×¨, ××“×“ ×•××™×›×•×ª ×—×™×–×•×™
summary_counts = df_melted_quality.groupby(['treatment', 'Feature', 'Metric', 'Forecast_Quality']).size().reset_index(name='Count') # Changed from Treatment_Alias to treatment

# × ×—×©×‘ ××ª ×¡×š ×”×ª××™× ×œ×›×œ ×˜×™×¤×•×œ, ×¤×™×¦'×¨ ×•××“×“ (×›×“×™ ×©× ×•×›×œ ×œ×—×©×‘ ××—×•×–×™×)
total_counts_per_group = summary_counts.groupby(['treatment', 'Feature', 'Metric'])['Count'].transform('sum').reset_index(name='Total') # Changed from Treatment_Alias to treatment
summary_counts['Total'] = total_counts_per_group['Total'] # Add Total column back

# × ×—×©×‘ ××ª ×”××—×•×–×™×
summary_counts['Percentage'] = (summary_counts['Count'] / summary_counts['Total']) * 100

# --- ×”×›× ×ª × ×ª×•× ×™× ×œ×’×¨×£ ---
# × ×“××’ ×©×™×”×™×• ×œ× ×• ×’× ×©×•×¨×•×ª ×¢×‘×•×¨ "Good" ×•×’× ×¢×‘×•×¨ "Bad" ×œ×›×œ ×§×‘×•×¦×”, ×’× ×× Count ×”×•× 0
# × ×‘× ×” DataFrame ×©×œ ×›×œ ×”×©×™×œ×•×‘×™× ×”××¤×©×¨×™×™× ×¢×‘×•×¨ MAPE ×•-P-Value ×‘×œ×‘×“
all_combinations = pd.MultiIndex.from_product(
    [df_raw['Treatment_Alias'].unique(), # Keep original for unique values from raw data
     df_raw['Feature'].unique(),
     ['MAPE', 'P-Value'], # ×¨×§ MAPE ×•-P-Value
     ['Good Forecast (MAPE â‰¤ 10%)', 'Bad Forecast (MAPE > 10%)',
      'Good Forecast (P â‰¤ 0.05)', 'Bad Forecast (P > 0.05)']],
    names=['Treatment_Alias', 'Feature', 'Metric', 'Forecast_Quality']
).to_frame(index=False)

# × ××–×’ ×¢× ×”× ×ª×•× ×™× ×©×—×™×©×‘× ×•
df_final_plot = pd.merge(all_combinations, summary_counts,
                         left_on=['Treatment_Alias', 'Feature', 'Metric', 'Forecast_Quality'], # Keep original for merge
                         right_on=['treatment', 'Feature', 'Metric', 'Forecast_Quality'], # Match with new 'treatment' column
                         how='left').fillna(0)

# Drop the redundant 'Treatment_Alias' column from df_final_plot if it exists after merge
if 'Treatment_Alias' in df_final_plot.columns and 'treatment' in df_final_plot.columns:
    df_final_plot = df_final_plot.drop(columns=['Treatment_Alias'])
df_final_plot = df_final_plot.rename(columns={'treatment': 'Treatment'}) # Rename 'treatment' to 'Treatment' for consistency in plot

# × × ×§×” ×¢×¨×›×™× ×©×œ× ×¨×œ×•×•× ×˜×™×™× (×›××• ×©×™×œ×•×‘×™× ×©×œ ×ª×•×•×™×•×ª Good/Bad ×©×’×•×™×•×ª ×¢× ××“×“×™×)
df_final_plot = df_final_plot[df_final_plot['Percentage'] > 0] # × ×¡× ×Ÿ ×©×•×¨×•×ª ×¨×™×§×•×ª

# × ×¡×“×¨ ××—×“×© ××ª ×”× ×ª×•× ×™× ×œ×¤×•×¨××˜ ×”××ª××™× ×œ×’×¨×£
df_final_plot['Forecast_Type'] = df_final_plot['Forecast_Quality'].apply(lambda x: 'Good Forecast' if 'Good' in x else 'Bad Forecast')
df_final_plot['Forecast_Label'] = df_final_plot['Forecast_Quality'] # Keep original label for legend

# × ×¡×›× ××ª ×”× ×ª×•× ×™× ×œ×¤×™ ×˜×™×¤×•×œ, ×¤×™×¦'×¨, ××“×“ ×•×¡×•×’ ×—×™×–×•×™
df_pivot = df_final_plot.pivot_table(index=['Treatment', 'Feature', 'Metric'], # Changed to 'Treatment'
                                     columns='Forecast_Type',
                                     values='Percentage',
                                     fill_value=0).reset_index()

df_pivot.columns.name = None
df_pivot = df_pivot.rename(columns={'Good Forecast': 'Good_Percentage', 'Bad Forecast': 'Bad_Percentage'})

df_plot_final_melted = df_pivot.melt(id_vars=['Treatment', 'Feature', 'Metric'], # Changed to 'Treatment'
                                     value_vars=['Good_Percentage', 'Bad_Percentage'],
                                     var_name='Forecast_Quality_Type',
                                     value_name='Percentage')

df_plot_final_melted['Forecast_Quality_Type'] = df_plot_final_melted['Forecast_Quality_Type'].replace({
    'Good_Percentage': 'Good Forecast',
    'Bad_Percentage': 'Bad Forecast'
})

# --- Forecasting Efficacy statistics (Chi-square test function) ---
def chi_square_test(df, metric):
    results = []
    features = df['Feature'].unique()

    for feature in features:
        contingency_table_data = df.loc[(df['Feature'] == feature) & (df['Metric'] == metric)]

        if len(contingency_table_data) > 0:
            contingency_table = pd.crosstab(
                contingency_table_data['treatment'], # Changed from Treatment_Alias to treatment
                contingency_table_data['Forecast_Quality_Type']
            )

            for col in ['Good Forecast', 'Bad Forecast']:
                if col not in contingency_table.columns:
                    contingency_table[col] = 0

            contingency_table = contingency_table[['Good Forecast', 'Bad Forecast']]

            all_treatment_aliases = df['treatment'].unique() # Changed from Treatment_Alias to treatment
            for alias in all_treatment_aliases:
                if alias not in contingency_table.index:
                    contingency_table.loc[alias] = [0, 0]
            contingency_table = contingency_table.reindex(all_treatment_aliases)

            if contingency_table.shape == (2, 2) and contingency_table.sum().sum() > 0:
                chi2, p, _, _ = chi2_contingency(contingency_table)
                results.append({'Feature': feature, 'Chi2': chi2, 'P-value': p})
            else:
                results.append({'Feature': feature, 'Chi2': None, 'P-value': None})
        else:
            results.append({'Feature': feature, 'Chi2': None, 'P-value': None})

    results_df = pd.DataFrame(results).dropna()

    if not results_df.empty:
        corrected = multipletests(results_df['P-value'], method='fdr_bh')
        results_df['Corrected_P-value'] = corrected[1]
        results_df['Significant'] = corrected[0]

    return results_df

# --- Calculate Chi-square results for P-Value ---
df_chi_square_input = df_melted_quality[df_melted_quality['Metric'] == 'P-Value'].copy()
df_chi_square_input['Forecast_Quality_Type'] = df_chi_square_input['Forecast_Quality'].apply(
    lambda x: 'Good Forecast' if 'Good' in x else 'Bad Forecast'
)
chi_square_results = chi_square_test(df_chi_square_input, 'P-Value')

# --- ×™×¦×™×¨×ª ×’×¨×£ ×”×¢××•×“×•×ª ×”××•×¢×¨× ---
# × ×’×“×™×¨ ××ª ×¡×“×¨ ×”××“×“×™× ×œ×”×¦×’×” (×¨×§ MAPE ×•-P-Value)
metric_order = ['P-Value']
feature_order = df_plot_final_melted['Feature'].unique()

g = sns.FacetGrid(df_plot_final_melted, col="Treatment", row="Metric", # Changed to 'Treatment'
                  height=4, aspect=1.5, sharey=True,
                  col_order=df_plot_final_melted['Treatment'].unique(), # Changed to 'Treatment'
                  row_order=metric_order)


g.map_dataframe(sns.barplot, x='Feature', y='Percentage', hue='Forecast_Quality_Type',
                palette={'Good Forecast': 'lightgreen', 'Bad Forecast': 'lightcoral'},
                order=feature_order,
                estimator=sum, errorbar=None)
g.add_legend(title="Forecast Quality")

g.set_axis_labels("Feature", "Percentage (%)")
g.set_titles(col_template="Treatment: {col_name}", row_template="Metric: {row_name}")

plt.suptitle('Percentage of Good vs. Bad Forecasts for P-Value per Feature and Treatment', y=1.02, fontsize=16)

plt.tight_layout() # Ensure tight layout without extra bottom margin

plt.show()

# --- Display Chi-square results in the console ---
print("\n--- Chi-square Test Results for P-Value ---")
print("Significance level (alpha): 0.05")
print("Significance is determined by 'Corrected_P-value' < alpha threshold.")
print(chi_square_results.to_markdown(index=False)) # Convert DataFrame to Markdown table

# --- ×©××™×¨×ª ×ª×•×¦××•×ª ×”-Chi-square ×œ×§×•×‘×¥ Excel ---
output_excel_path = '/content/chi_square_results.xlsx'
try:
    chi_square_results.to_excel(output_excel_path, index=False)
    print(f"\nChi-square test results saved successfully to: {output_excel_path}")
    print("You can find the file in the Colab file browser (folder icon on the left).")
except Exception as e:
    print(f"Error saving the Excel file: {e}")